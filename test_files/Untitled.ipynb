{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16744c1b-8c68-4cd0-a9ad-0e690210b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7632121-8c93-404f-a164-d53070f7bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "# IMAGE_PATH = \"weights/dog-3.jpeg\"\n",
    "IMAGE_PATH = \"ant_images/IMG_5793.jpeg\"\n",
    "# TEXT_PROMPT = \"chair . person . dog .\"\n",
    "TEXT_PROMPT = \"ant\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b449-f123-4ebe-be6d-2d85ead5374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dfd46d-a0ad-4888-b726-ab74878c9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410eab37-ba18-44ad-8ebf-a3db45d950ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[176, 176, 190],\n",
       "        [179, 179, 193],\n",
       "        [182, 181, 197],\n",
       "        ...,\n",
       "        [235, 233, 233],\n",
       "        [232, 230, 230],\n",
       "        [233, 231, 231]],\n",
       "\n",
       "       [[179, 179, 193],\n",
       "        [181, 181, 195],\n",
       "        [182, 181, 197],\n",
       "        ...,\n",
       "        [235, 233, 233],\n",
       "        [234, 232, 232],\n",
       "        [235, 233, 233]],\n",
       "\n",
       "       [[181, 180, 196],\n",
       "        [181, 180, 196],\n",
       "        [181, 180, 196],\n",
       "        ...,\n",
       "        [235, 233, 233],\n",
       "        [235, 233, 233],\n",
       "        [238, 236, 236]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 81,  64, 128],\n",
       "        [ 78,  61, 125],\n",
       "        [ 75,  58, 121],\n",
       "        ...,\n",
       "        [253, 251, 250],\n",
       "        [255, 253, 252],\n",
       "        [255, 254, 253]],\n",
       "\n",
       "       [[ 85,  68, 132],\n",
       "        [ 80,  63, 127],\n",
       "        [ 75,  58, 121],\n",
       "        ...,\n",
       "        [253, 251, 250],\n",
       "        [255, 253, 252],\n",
       "        [255, 254, 253]],\n",
       "\n",
       "       [[ 92,  75, 139],\n",
       "        [ 84,  67, 131],\n",
       "        [ 75,  58, 121],\n",
       "        ...,\n",
       "        [253, 251, 250],\n",
       "        [254, 252, 251],\n",
       "        [255, 254, 253]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37051ee-5362-4315-a034-67b20ea3db3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
